# generated by datamodel-codegen:
#   filename:  benchmark-data-schema.json
#   timestamp: 2025-12-24T15:07:25+00:00

from __future__ import annotations

from pydantic import AwareDatetime, BaseModel, Field, RootModel


class Dimension(RootModel[int]):
    root: int = Field(..., ge=2)


class Metadata(BaseModel):
    max_iterations: int = Field(..., ge=1)
    n_runs: int = Field(..., ge=1)
    dimensions: list[Dimension]
    timestamp: AwareDatetime
    python_version: str | None = None
    numpy_version: str | None = None


class History(BaseModel):
    best_fitness: list[float] | None = None
    mean_fitness: list[float] | None = None


class Run(BaseModel):
    best_fitness: float
    best_solution: list[float]
    n_evaluations: int
    history: History | None = None


class Statistics(BaseModel):
    mean_fitness: float
    std_fitness: float
    min_fitness: float
    max_fitness: float
    median_fitness: float
    q1_fitness: float | None = None
    q3_fitness: float | None = None


class Benchmarks(BaseModel):
    runs: list[Run]
    statistics: Statistics
    success_rate: float = Field(..., ge=0.0, le=1.0)


class BenchmarkDataSchema(BaseModel):
    metadata: Metadata
    benchmarks: dict[str, dict[str, dict[str, Benchmarks]]]
